# utils/code_cleanup.py
"""
ä»£ç æ¸…ç†å’Œé‡å¤æ£€æµ‹å·¥å…·
å¸®åŠ©è¯†åˆ«å’Œæ¸…ç†é¡¹ç›®ä¸­çš„é‡å¤ä»£ç ã€æœªä½¿ç”¨æ–‡ä»¶ç­‰
"""
import os
import ast
import re
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any
from dataclasses import dataclass, field
from collections import defaultdict, Counter
import importlib.util

from loguru import logger


@dataclass
class DuplicateCode:
    """é‡å¤ä»£ç ä¿¡æ¯"""
    files: List[str]
    lines: List[Tuple[int, int]]  # (start_line, end_line)
    code: str
    similarity: float
    type: str  # "function", "class", "block"


@dataclass
class UnusedFile:
    """æœªä½¿ç”¨æ–‡ä»¶ä¿¡æ¯"""
    file_path: str
    reason: str
    size: int
    last_modified: float


@dataclass
class CodeIssue:
    """ä»£ç é—®é¢˜"""
    file_path: str
    line_number: int
    issue_type: str
    description: str
    severity: str  # "low", "medium", "high"


@dataclass
class CleanupReport:
    """æ¸…ç†æŠ¥å‘Š"""
    duplicate_codes: List[DuplicateCode] = field(default_factory=list)
    unused_files: List[UnusedFile] = field(default_factory=list)
    code_issues: List[CodeIssue] = field(default_factory=list)
    total_files_scanned: int = 0
    total_lines_scanned: int = 0
    duplicate_lines: int = 0
    potential_savings: Dict[str, Any] = field(default_factory=dict)


class CodeAnalyzer:
    """ä»£ç åˆ†æå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.python_files: List[Path] = []
        self.import_graph: Dict[str, Set[str]] = defaultdict(set)
        self.function_hashes: Dict[str, List[Tuple[str, int, str]]] = defaultdict(list)
        self.class_hashes: Dict[str, List[Tuple[str, int, str]]] = defaultdict(list)

        # æ’é™¤çš„ç›®å½•å’Œæ–‡ä»¶
        self.exclude_dirs = {'.git', '__pycache__', '.pytest_cache', 'venv', '.env', 'node_modules'}
        self.exclude_files = {'__init__.py'}  # æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¸ºç©ºä½†ä»ç„¶éœ€è¦

    def scan_project(self):
        """æ‰«æé¡¹ç›®æ–‡ä»¶"""
        logger.info(f"ğŸ“ æ‰«æé¡¹ç›®: {self.project_root}")

        self.python_files = []
        for file_path in self.project_root.rglob("*.py"):
            # è·³è¿‡æ’é™¤çš„ç›®å½•
            if any(excluded in file_path.parts for excluded in self.exclude_dirs):
                continue
            self.python_files.append(file_path)

        logger.info(f"æ‰¾åˆ° {len(self.python_files)} ä¸ªPythonæ–‡ä»¶")

    def parse_file(self, file_path: Path) -> Tuple[ast.AST, str]:
        """è§£æPythonæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content, filename=str(file_path))
            return tree, content
        except Exception as e:
            logger.warning(f"è§£ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None, ""

    def extract_imports(self, tree: ast.AST, file_path: Path) -> Set[str]:
        """æå–å¯¼å…¥ä¿¡æ¯"""
        imports = set()

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module)

        return imports

    def extract_functions_and_classes(self, tree: ast.AST, content: str, file_path: Path):
        """æå–å‡½æ•°å’Œç±»å®šä¹‰"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_code = ast.get_source_segment(content, node)
                if func_code:
                    func_hash = hashlib.md5(func_code.encode()).hexdigest()
                    self.function_hashes[func_hash].append((str(file_path), node.lineno, func_code))

            elif isinstance(node, ast.ClassDef):
                class_code = ast.get_source_segment(content, node)
                if class_code:
                    class_hash = hashlib.md5(class_code.encode()).hexdigest()
                    self.class_hashes[class_hash].append((str(file_path), node.lineno, class_code))

    def build_import_graph(self):
        """æ„å»ºå¯¼å…¥ä¾èµ–å›¾"""
        logger.info("ğŸ”— æ„å»ºå¯¼å…¥ä¾èµ–å›¾...")

        for file_path in self.python_files:
            tree, content = self.parse_file(file_path)
            if tree:
                imports = self.extract_imports(tree, file_path)
                self.import_graph[str(file_path)] = imports
                self.extract_functions_and_classes(tree, content, file_path)

    def find_duplicate_functions(self) -> List[DuplicateCode]:
        """æŸ¥æ‰¾é‡å¤å‡½æ•°"""
        duplicates = []

        for func_hash, locations in self.function_hashes.items():
            if len(locations) > 1:
                duplicate = DuplicateCode(
                    files=[loc[0] for loc in locations],
                    lines=[(loc[1], loc[1] + loc[2].count('\n')) for loc in locations],
                    code=locations[0][2],
                    similarity=1.0,  # å®Œå…¨åŒ¹é…
                    type="function"
                )
                duplicates.append(duplicate)

        logger.info(f"æ‰¾åˆ° {len(duplicates)} ä¸ªé‡å¤å‡½æ•°")
        return duplicates

    def find_duplicate_classes(self) -> List[DuplicateCode]:
        """æŸ¥æ‰¾é‡å¤ç±»"""
        duplicates = []

        for class_hash, locations in self.class_hashes.items():
            if len(locations) > 1:
                duplicate = DuplicateCode(
                    files=[loc[0] for loc in locations],
                    lines=[(loc[1], loc[1] + loc[2].count('\n')) for loc in locations],
                    code=locations[0][2],
                    similarity=1.0,
                    type="class"
                )
                duplicates.append(duplicate)

        logger.info(f"æ‰¾åˆ° {len(duplicates)} ä¸ªé‡å¤ç±»")
        return duplicates

    def find_unused_files(self) -> List[UnusedFile]:
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„æ–‡ä»¶"""
        logger.info("ğŸ” æŸ¥æ‰¾æœªä½¿ç”¨æ–‡ä»¶...")

        # è·å–æ‰€æœ‰è¢«å¯¼å…¥çš„æ¨¡å—
        imported_modules = set()
        for imports in self.import_graph.values():
            imported_modules.update(imports)

        # è½¬æ¢ä¸ºæ–‡ä»¶è·¯å¾„æ ¼å¼
        imported_files = set()
        for module in imported_modules:
            # ç®€å•çš„æ¨¡å—ååˆ°æ–‡ä»¶è·¯å¾„è½¬æ¢
            module_parts = module.split('.')
            for file_path in self.python_files:
                if any(part in str(file_path) for part in module_parts):
                    imported_files.add(str(file_path))

        unused_files = []
        for file_path in self.python_files:
            file_str = str(file_path)

            # è·³è¿‡ä¸»ç¨‹åºå’Œé…ç½®æ–‡ä»¶
            if any(name in file_str for name in
                   ['main.py', 'settings.py', 'config.py', '__init__.py']):
                continue

            # æ£€æŸ¥æ˜¯å¦è¢«å¯¼å…¥æˆ–å¼•ç”¨
            if file_str not in imported_files:
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åœ¨ä»£ç ä¸­è¢«å­—ç¬¦ä¸²å¼•ç”¨
                is_referenced = False
                for other_file in self.python_files:
                    if other_file == file_path:
                        continue

                    try:
                        with open(other_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            if file_path.stem in content:
                                is_referenced = True
                                break
                    except Exception:
                        continue

                if not is_referenced:
                    stat = os.stat(file_path)
                    unused_files.append(UnusedFile(
                        file_path=file_str,
                        reason="æœªè¢«å¯¼å…¥æˆ–å¼•ç”¨",
                        size=stat.st_size,
                        last_modified=stat.st_mtime
                    ))

        logger.info(f"æ‰¾åˆ° {len(unused_files)} ä¸ªå¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶")
        return unused_files

    def find_code_issues(self) -> List[CodeIssue]:
        """æŸ¥æ‰¾ä»£ç é—®é¢˜"""
        logger.info("ğŸ› æŸ¥æ‰¾ä»£ç é—®é¢˜...")

        issues = []

        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

                for i, line in enumerate(lines, 1):
                    line_stripped = line.strip()

                    # æ£€æŸ¥é•¿è¡Œ
                    if len(line) > 120:
                        issues.append(CodeIssue(
                            file_path=str(file_path),
                            line_number=i,
                            issue_type="long_line",
                            description=f"è¡Œè¿‡é•¿ ({len(line)} å­—ç¬¦)",
                            severity="low"
                        ))

                    # æ£€æŸ¥TODO/FIXME
                    if any(
                        keyword in line_stripped.upper() for keyword in ['TODO', 'FIXME', 'XXX']):
                        issues.append(CodeIssue(
                            file_path=str(file_path),
                            line_number=i,
                            issue_type="todo",
                            description="æœªå®Œæˆçš„TODO/FIXME",
                            severity="medium"
                        ))

                    # æ£€æŸ¥é‡å¤çš„è£…é¥°å™¨æ¨¡å¼
                    if '@' in line and any(
                        pattern in line for pattern in ['cached', 'retry', 'method_cache']):
                        # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„è£…é¥°å™¨æ¨¡å¼
                        if i > 1 and '@' in lines[i - 2] and any(
                            pattern in lines[i - 2] for pattern in ['cached', 'retry']):
                            issues.append(CodeIssue(
                                file_path=str(file_path),
                                line_number=i,
                                issue_type="duplicate_decorator",
                                description="å¯èƒ½å­˜åœ¨é‡å¤çš„è£…é¥°å™¨æ¨¡å¼",
                                severity="medium"
                            ))

            except Exception as e:
                logger.warning(f"æ£€æŸ¥æ–‡ä»¶é—®é¢˜å¤±è´¥ {file_path}: {e}")

        logger.info(f"æ‰¾åˆ° {len(issues)} ä¸ªä»£ç é—®é¢˜")
        return issues


class CodeCleanupTool:
    """ä»£ç æ¸…ç†å·¥å…·"""

    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.analyzer = CodeAnalyzer(project_root)

    def analyze_project(self) -> CleanupReport:
        """åˆ†æé¡¹ç›®"""
        logger.info("ğŸ” å¼€å§‹é¡¹ç›®åˆ†æ...")

        # æ‰«æé¡¹ç›®
        self.analyzer.scan_project()

        # æ„å»ºä¾èµ–å›¾
        self.analyzer.build_import_graph()

        # æŸ¥æ‰¾é—®é¢˜
        duplicate_functions = self.analyzer.find_duplicate_functions()
        duplicate_classes = self.analyzer.find_duplicate_classes()
        unused_files = self.analyzer.find_unused_files()
        code_issues = self.analyzer.find_code_issues()

        # ç»Ÿè®¡ä¿¡æ¯
        total_lines = 0
        duplicate_lines = 0

        for file_path in self.analyzer.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = len(f.readlines())
                    total_lines += lines
            except Exception:
                continue

        for dup in duplicate_functions + duplicate_classes:
            duplicate_lines += sum(end - start + 1 for start, end in dup.lines)

        # ç”ŸæˆæŠ¥å‘Š
        report = CleanupReport(
            duplicate_codes=duplicate_functions + duplicate_classes,
            unused_files=unused_files,
            code_issues=code_issues,
            total_files_scanned=len(self.analyzer.python_files),
            total_lines_scanned=total_lines,
            duplicate_lines=duplicate_lines,
            potential_savings={
                "duplicate_code_lines": duplicate_lines,
                "unused_files_size": sum(f.size for f in unused_files),
                "unused_files_count": len(unused_files)
            }
        )

        logger.info("âœ… é¡¹ç›®åˆ†æå®Œæˆ")
        return report

    def generate_cleanup_script(self, report: CleanupReport, output_file: Path):
        """ç”Ÿæˆæ¸…ç†è„šæœ¬"""
        logger.info(f"ğŸ“ ç”Ÿæˆæ¸…ç†è„šæœ¬: {output_file}")

        script_content = f"""#!/usr/bin/env python3
# è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç æ¸…ç†è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now()}

import os
import shutil
from pathlib import Path

def backup_file(file_path):
    \"\"\"å¤‡ä»½æ–‡ä»¶\"\"\"
    backup_path = str(file_path) + '.backup'
    shutil.copy2(file_path, backup_path)
    print(f"å·²å¤‡ä»½: {{file_path}} -> {{backup_path}}")

def remove_unused_files():
    \"\"\"ç§»é™¤æœªä½¿ç”¨çš„æ–‡ä»¶\"\"\"
    unused_files = [
"""

        for unused_file in report.unused_files:
            script_content += f'        "{unused_file.file_path}",\n'

        script_content += """    ]

    for file_path in unused_files:
        if os.path.exists(file_path):
            print(f"ç§»é™¤æœªä½¿ç”¨æ–‡ä»¶: {file_path}")
            # å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„è¡Œæ¥å®é™…åˆ é™¤æ–‡ä»¶
            # backup_file(file_path)
            # os.remove(file_path)

def show_duplicate_code():
    \"\"\"æ˜¾ç¤ºé‡å¤ä»£ç \"\"\"
    duplicates = [
"""

        for i, dup in enumerate(report.duplicate_codes):
            script_content += f"""        {{
            "id": {i},
            "type": "{dup.type}",
            "files": {dup.files},
            "lines": {dup.lines}
        }},
"""

        script_content += """    ]

    for dup in duplicates:
        print(f"é‡å¤{dup['type']}: {dup['files']}")
        print(f"è¡Œæ•°: {dup['lines']}")
        print("-" * 50)

if __name__ == "__main__":
    print("ğŸ§¹ ä»£ç æ¸…ç†è„šæœ¬")
    print("="*50)

    print("\\nğŸ“Š åˆ†ææŠ¥å‘Š:")
    print(f"æ‰«ææ–‡ä»¶: {report.total_files_scanned}")
    print(f"æ‰«æè¡Œæ•°: {report.total_lines_scanned}")
    print(f"é‡å¤ä»£ç è¡Œæ•°: {report.duplicate_lines}")
    print(f"æœªä½¿ç”¨æ–‡ä»¶: {len(report.unused_files)}")
    print(f"ä»£ç é—®é¢˜: {len(report.code_issues)}")

    print("\\nğŸ” é‡å¤ä»£ç :")
    show_duplicate_code()

    print("\\nğŸ—‘ï¸  æœªä½¿ç”¨æ–‡ä»¶:")
    remove_unused_files()

    print("\\nâš ï¸  æ³¨æ„: è¯·ä»”ç»†æ£€æŸ¥åå†æ‰§è¡Œå®é™…çš„æ–‡ä»¶åˆ é™¤æ“ä½œï¼")
"""

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script_content)

        # è®¾ç½®æ‰§è¡Œæƒé™
        output_file.chmod(0o755)

        logger.info(f"æ¸…ç†è„šæœ¬å·²ç”Ÿæˆ: {output_file}")

    def generate_report(self, report: CleanupReport, output_file: Path):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        logger.info(f"ğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š: {output_file}")

        report_content = f"""# ä»£ç æ¸…ç†åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now()}
é¡¹ç›®è·¯å¾„: {self.project_root}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- æ‰«ææ–‡ä»¶æ•°: {report.total_files_scanned}
- æ‰«æè¡Œæ•°: {report.total_lines_scanned:,}
- é‡å¤ä»£ç è¡Œæ•°: {report.duplicate_lines:,}
- æœªä½¿ç”¨æ–‡ä»¶æ•°: {len(report.unused_files)}
- ä»£ç é—®é¢˜æ•°: {len(report.code_issues)}

## ğŸ”„ é‡å¤ä»£ç 

æ‰¾åˆ° {len(report.duplicate_codes)} å¤„é‡å¤ä»£ç ï¼š

"""

        for i, dup in enumerate(report.duplicate_codes, 1):
            report_content += f"""### é‡å¤ä»£ç  #{i}

- ç±»å‹: {dup.type}
- ç›¸ä¼¼åº¦: {dup.similarity:.1%}
- æ¶‰åŠæ–‡ä»¶:
"""
            for file_path, (start, end) in zip(dup.files, dup.lines):
                report_content += f"  - `{file_path}` (è¡Œ {start}-{end})\n"

            report_content += f"""
```python
{dup.code[:200]}{'...' if len(dup.code) > 200 else ''}
```

"""

        report_content += f"""## ğŸ—‘ï¸ æœªä½¿ç”¨æ–‡ä»¶

æ‰¾åˆ° {len(report.unused_files)} ä¸ªå¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶ï¼š

"""

        for unused in sorted(report.unused_files, key=lambda x: x.size, reverse=True):
            size_kb = unused.size / 1024
            report_content += f"- `{unused.file_path}` ({size_kb:.1f} KB) - {unused.reason}\n"

        report_content += f"""
## ğŸ› ä»£ç é—®é¢˜

æ‰¾åˆ° {len(report.code_issues)} ä¸ªä»£ç é—®é¢˜ï¼š

"""

        issue_groups = defaultdict(list)
        for issue in report.code_issues:
            issue_groups[issue.issue_type].append(issue)

        for issue_type, issues in issue_groups.items():
            report_content += f"""### {issue_type.replace('_', ' ').title()}

"""
            for issue in issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report_content += f"- `{issue.file_path}:{issue.line_number}` - {issue.description}\n"

            if len(issues) > 10:
                report_content += f"- ... è¿˜æœ‰ {len(issues) - 10} ä¸ªç±»ä¼¼é—®é¢˜\n"

        report_content += f"""
## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### é‡å¤ä»£ç ä¼˜åŒ–
1. å°†é‡å¤çš„å‡½æ•°æå–åˆ°å·¥å…·æ¨¡å—ä¸­
2. åˆ›å»ºåŸºç±»æ¥æ¶ˆé™¤é‡å¤çš„ç±»å®šä¹‰
3. ä½¿ç”¨è£…é¥°å™¨æ¨¡å¼ç»Ÿä¸€é‡å¤çš„åŠŸèƒ½

### æ–‡ä»¶æ¸…ç†
1. åˆ é™¤æœªä½¿ç”¨çš„æ–‡ä»¶ï¼ˆè¯·å…ˆç¡®è®¤ï¼‰
2. åˆå¹¶åŠŸèƒ½ç›¸ä¼¼çš„å°æ–‡ä»¶
3. é‡æ–°ç»„ç»‡ç›®å½•ç»“æ„

### ä»£ç è´¨é‡
1. ä¿®å¤é•¿è¡Œé—®é¢˜ï¼Œæé«˜å¯è¯»æ€§
2. å¤„ç†TODO/FIXMEæ ‡è®°
3. ç»Ÿä¸€ä»£ç é£æ ¼å’Œæ¨¡å¼

## ğŸ“ˆ é¢„æœŸæ”¶ç›Š

- å‡å°‘ä»£ç è¡Œæ•°: ~{report.duplicate_lines:,} è¡Œ
- å‡å°‘æ–‡ä»¶æ•°: ~{len(report.unused_files)} ä¸ª
- èŠ‚çœç£ç›˜ç©ºé—´: ~{report.potential_savings.get('unused_files_size', 0) / 1024:.1f} KB
"""

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        logger.info(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ä»£ç æ¸…ç†å’Œåˆ†æå·¥å…·")
    parser.add_argument("project_path", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--output-dir", default="cleanup_results", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--generate-script", action="store_true", help="ç”Ÿæˆæ¸…ç†è„šæœ¬")

    args = parser.parse_args()

    project_path = Path(args.project_path)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    # åˆ›å»ºæ¸…ç†å·¥å…·
    cleanup_tool = CodeCleanupTool(project_path)

    # åˆ†æé¡¹ç›®
    report = cleanup_tool.analyze_project()

    # ç”ŸæˆæŠ¥å‘Š
    report_file = output_dir / "cleanup_report.md"
    cleanup_tool.generate_report(report, report_file)

    # ç”Ÿæˆæ¸…ç†è„šæœ¬
    if args.generate_script:
        script_file = output_dir / "cleanup_script.py"
        cleanup_tool.generate_cleanup_script(report, script_file)

    print(f"""
ğŸ‰ åˆ†æå®Œæˆï¼

ğŸ“Š ç»Ÿè®¡ç»“æœ:
- æ‰«ææ–‡ä»¶: {report.total_files_scanned}
- é‡å¤ä»£ç : {len(report.duplicate_codes)} å¤„
- æœªä½¿ç”¨æ–‡ä»¶: {len(report.unused_files)} ä¸ª
- ä»£ç é—®é¢˜: {len(report.code_issues)} ä¸ª

ğŸ“ è¾“å‡ºæ–‡ä»¶:
- è¯¦ç»†æŠ¥å‘Š: {report_file}
{f"- æ¸…ç†è„šæœ¬: {output_dir / 'cleanup_script.py'}" if args.generate_script else ""}

ğŸ’¡ å»ºè®®: è¯·ä»”ç»†æ£€æŸ¥æŠ¥å‘Šåå†æ‰§è¡Œä»»ä½•æ–‡ä»¶åˆ é™¤æ“ä½œã€‚
    """)


if __name__ == "__main__":
    main()
